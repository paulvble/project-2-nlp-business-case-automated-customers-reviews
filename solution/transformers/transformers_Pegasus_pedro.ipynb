{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c70886-ee5b-4e6f-9cda-9bc7eab63032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Text split into 2 chunks.\n",
      "Processing batch 1 with 2 chunks.\n",
      "Batch summaries: ['After discarding and getting rid of broken cd cases, broken cds, and selecting those ones we really like, this binder turned up to be an excellent option to store our favourite cds and dvds and keep them in a small space at our living room, giving us the choice to donate or get rid of those cds towers that took a lot of room, despite looking nice. Cons: May be not trendy looking as newer, but still sets well on my kitchen island Pros: I love the Alexa series so when this one went on sale I had to get it with a camera. Pros: Works good like Alexia Cons: setting it up is a, HASLE had to call verizon three times Other Thoughts: I WOULD RECOMMEND', 'Pros: I WOULD recommend anyone who was going to purchase anything, newegg is the BEST PLACE TO LOOK! Cons: No compaints']\n",
      "Combined summary length: 177\n",
      "Primary Category: Electronics, Star Rating: 5\n",
      "Summary: After discarding and getting rid of broken cd cases, broken cds, and selecting those ones we really like, this binder turned up to be an excellent option to store our favourite cds and dvds and keep them in a small space at our living room, giving us the choice to donate or get rid of those cds towers that took a lot of room, despite looking nice. Cons: May be not trendy looking as newer, but still sets well on my kitchen island Pros: I love the Alexa series so when this one went on sale I had to get it with a camera. Pros: Works good like Alexia Cons: setting it up is a, HASLE had to call verizon three times Other Thoughts: I WOULD RECOMMEND Pros: I WOULD recommend anyone who was going to purchase anything, newegg is the BEST PLACE TO LOOK! Cons: No compaints\n",
      "\n",
      "Primary Category: Electronics, Star Rating: 5\n",
      "Original Reviews: Great case to keep everything in its place! My husband love it!!!! Holds a lot of cds! After discarding and getting rid of broken cd cases, broken cds, and selecting those ones we really like, this binder turned up to be an excellent option to store our favourite cds and dvds and keep them in a small space at our living room, giving us the choice to donate or get rid of those cds towers that took a lot of room, despite looking nice. And because you can turn the pages, you can spot the cd you want to play without the hassle of taking it from a case that is falling apart. After storing them cds, all the cases and printed covers went straight to the recycling centre. We have a small version to put those cds my husband would like to listen to whenever he must drive away for work. A few dollars more, but I am boycotting amazon Pros: Standard Echo. Cons: Older generation Echo. Other Thoughts: Arrived on time and was new in box. Pros: Got it for under 50, much impressive sound than 2nd gen. H...\n",
      "Summary: After discarding and getting rid of broken cd cases, broken cds, and selecting those ones we really like, this binder turned up to be an excellent option to store our favourite cds and dvds and keep them in a small space at our living room, giving us the choice to donate or get rid of those cds towers that took a lot of room, despite looking nice. Cons: May be not trendy looking as newer, but still sets well on my kitchen island Pros: I love the Alexa series so when this one went on sale I had to get it with a camera. Pros: Works good like Alexia Cons: setting it up is a, HASLE had to call verizon three times Other Thoughts: I WOULD RECOMMEND Pros: I WOULD recommend anyone who was going to purchase anything, newegg is the BEST PLACE TO LOOK! Cons: No compaints\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the dataset\n",
    "data_path = Path('..') / 'data' / 'data.csv'\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"The file at {data_path} does not exist.\")\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "data = data[[\"primaryCategories\", \"reviews.text\", \"reviews.rating\"]]\n",
    "\n",
    "# Drop rows with missing values in the specified columns (if any)\n",
    "data.dropna(subset=['primaryCategories', 'reviews.text', 'reviews.rating'], inplace=True)\n",
    "\n",
    "# Convert ratings to string to ensure consistent grouping\n",
    "data['reviews.rating'] = data['reviews.rating'].astype(str)\n",
    "\n",
    "# Filter the data for a specific category and 5-star rating\n",
    "category = \"Electronics\"  # Example category, change as needed\n",
    "rating = \"5\"\n",
    "filtered_data = data[(data['primaryCategories'] == category) & (data['reviews.rating'] == rating)]\n",
    "\n",
    "# For testing, use a smaller subset\n",
    "filtered_data = filtered_data.head(10)  # Use only the first 10 rows for testing\n",
    "\n",
    "# Concatenate all reviews within the selected group\n",
    "grouped_reviews = filtered_data.groupby(['primaryCategories', 'reviews.rating'])['reviews.text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Load the PEGASUS model and tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "# Check if GPU is available and use it\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to split text into chunks based on maximum length\n",
    "def split_into_chunks(text, max_length):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for word in words:\n",
    "        word_length = len(tokenizer.encode(word, add_special_tokens=False))\n",
    "        if current_length + word_length <= max_length:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Function to summarize text by chunking with batch processing\n",
    "def summarize_text(text, max_chunk_length=512, summary_max_length=150, summary_min_length=30, batch_size=2):  # Reduced batch size for testing\n",
    "    chunks = split_into_chunks(text, max_chunk_length)\n",
    "    print(f\"Text split into {len(chunks)} chunks.\")\n",
    "    \n",
    "    chunk_summaries = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} with {len(batch)} chunks.\")\n",
    "        inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=max_chunk_length)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Generate summaries for the batch\n",
    "        summary_ids = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=summary_max_length, \n",
    "            min_length=summary_min_length, \n",
    "            length_penalty=1.0,  # Adjust length penalty\n",
    "            num_beams=6,  # Increase number of beams\n",
    "            no_repeat_ngram_size=3,  # Prevent repetition of phrases\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode the summaries\n",
    "        summaries = [tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids]\n",
    "        print(f\"Batch summaries: {summaries}\")\n",
    "        chunk_summaries.extend(summaries)\n",
    "    \n",
    "    # Combine the summaries and summarize them if necessary\n",
    "    combined_summary = ' '.join(chunk_summaries)\n",
    "    print(f\"Combined summary length: {len(tokenizer.encode(combined_summary))}\")\n",
    "    if len(tokenizer.encode(combined_summary)) > max_chunk_length:\n",
    "        final_summary = summarize_text(combined_summary, max_chunk_length, summary_max_length, summary_min_length, batch_size)\n",
    "    else:\n",
    "        final_summary = combined_summary\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "# Function to remove repetitive phrases\n",
    "def remove_repetitive_phrases(text):\n",
    "    sentences = text.split('. ')\n",
    "    unique_sentences = []\n",
    "    seen_sentences = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen_sentences:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen_sentences.add(sentence)\n",
    "    \n",
    "    return '. '.join(unique_sentences)\n",
    "\n",
    "# Apply summarization to the selected group of reviews\n",
    "grouped_reviews['summary'] = grouped_reviews['reviews.text'].apply(lambda x: remove_repetitive_phrases(summarize_text(x)))\n",
    "\n",
    "# Display the summary for the selected combination\n",
    "for idx, row in grouped_reviews.iterrows():\n",
    "    print(f\"Primary Category: {row['primaryCategories']}, Star Rating: {row['reviews.rating']}\")\n",
    "    print(f\"Summary: {row['summary']}\\n\")\n",
    "\n",
    "# Visualize the reviews and their summaries\n",
    "for idx, row in grouped_reviews.iterrows():\n",
    "    print(f\"Primary Category: {row['primaryCategories']}, Star Rating: {row['reviews.rating']}\")\n",
    "    print(f\"Original Reviews: {row['reviews.text'][:1000]}...\")  # Print only the first 1000 characters of the original reviews\n",
    "    print(f\"Summary: {row['summary']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bb418-a0fc-4ff7-bf4b-6f3a27004f33",
   "metadata": {},
   "source": [
    "## Fine-tuning Pegasus model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853434e-ac25-4c78-bbae-119a0234493d",
   "metadata": {},
   "source": [
    "### 1 hour fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a122a82-91a9-48e5-a72e-d714837ee456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pedro\\anaconda3\\envs\\PytorchCudaEnv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8ebb0807274106af7a8830f2da059e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\anaconda3\\envs\\PytorchCudaEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcfc5e12a6e44a7a9097692ba62f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2834 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\anaconda3\\envs\\PytorchCudaEnv\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='377' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [377/756 43:02 < 43:30, 0.15 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.042816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.021437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.019021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data_path = Path('..') / 'data' / 'data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Select and preprocess relevant columns\n",
    "data = data[[\"reviews.text\"]].dropna()\n",
    "data['reviews.text'] = data['reviews.text'].astype(str)\n",
    "\n",
    "# Assume summaries are not available; generate synthetic summaries for demonstration\n",
    "# In practice, you would use actual summaries if available\n",
    "data['summary'] = data['reviews.text'].apply(lambda x: x[:512])  # Example: Use the first 512 characters as a summary\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test']\n",
    "})\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "# Tokenize the input text and summaries\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['reviews.text'], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments with max_steps set to train for approximately 1 hour\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    max_steps=756,  # Train for approximately 1 hour\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,  # Save less frequently\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-pegasus\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-pegasus\")\n",
    "\n",
    "# Summarize the reviews\n",
    "def summarize_text(text, max_length=256, num_beams=8, length_penalty=0.8):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=max_length, \n",
    "        num_beams=num_beams, \n",
    "        length_penalty=length_penalty, \n",
    "        forced_eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply summarization to the dataset\n",
    "data['summary'] = data['reviews.text'].apply(summarize_text)\n",
    "for idx, row in data.iterrows():\n",
    "    print(f\"Original Review: {row['reviews.text'][:500]}...\")\n",
    "    print(f\"Summary: {row['summary']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78c314-21c2-4099-a036-2075d35a7eb1",
   "metadata": {},
   "source": [
    "### 8 hours fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f673cf-b900-49ee-a35b-692e3aa87efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data_path = Path('..') / 'data' / 'data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Select and preprocess relevant columns\n",
    "data = data[[\"reviews.text\"]].dropna()\n",
    "data['reviews.text'] = data['reviews.text'].astype(str)\n",
    "\n",
    "# Assume summaries are not available; generate synthetic summaries for demonstration\n",
    "# In practice, you would use actual summaries if available\n",
    "data['summary'] = data['reviews.text'].apply(lambda x: x[:512])  # Example: Use the first 512 characters as a summary\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'test': dataset['test']\n",
    "})\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "# Tokenize the input text and summaries\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['reviews.text'], max_length=512, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments with max_steps set to train for approximately 8 hours\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,  # Evaluate every 500 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    max_steps=6048,  # Train for approximately 8 hours\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,  # Save less frequently\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-pegasus\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-pegasus\")\n",
    "\n",
    "# Summarize the reviews\n",
    "def summarize_text(text, max_length=256, num_beams=8, length_penalty=0.8):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=max_length, \n",
    "        num_beams=num_beams, \n",
    "        length_penalty=length_penalty, \n",
    "        forced_eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply summarization to the dataset\n",
    "data['summary'] = data['reviews.text'].apply(summarize_text)\n",
    "for idx, row in data.iterrows():\n",
    "    print(f\"Original Review: {row['reviews.text'][:500]}...\")\n",
    "    print(f\"Summary: {row['summary']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e39f85-7108-439d-8b9b-ef7b35bc0b70",
   "metadata": {},
   "source": [
    "## Loading & Summarizing with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3402367-1891-4225-a16d-3c0ab763e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"./fine-tuned-pegasus\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"./fine-tuned-pegasus\")\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, max_length=150, min_length=30):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"longest\")\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Summarize the reviews\n",
    "data['summary'] = data['reviews.text'].apply(summarize_text)\n",
    "for idx, row in data.iterrows():\n",
    "    print(f\"Original Review: {row['reviews.text'][:500]}...\")\n",
    "    print(f\"Summary: {row['summary']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
